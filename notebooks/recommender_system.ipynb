{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macpro/anaconda3/envs/mlenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 2.61MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 27.0kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 136kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 440M/440M [01:09<00:00, 6.32MB/s] \n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[1;32m     17\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     18\u001b[0m     output_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./results\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m     num_train_epochs\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m )\n\u001b[1;32m     25\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     26\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     27\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m---> 28\u001b[0m     train_dataset\u001b[39m=\u001b[39mtrain_dataset,\n\u001b[1;32m     29\u001b[0m     eval_dataset\u001b[39m=\u001b[39mval_dataset,\n\u001b[1;32m     30\u001b[0m )\n\u001b[1;32m     32\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     34\u001b[0m \u001b[39m# Get embeddings and make recommendations\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from .autonotebook import tqdm as notebook_tqdm \n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# Encode input text\n",
    "input_text = \"The user prefers a red, 2023, automatic transmission, electric car with a budget of $50,000.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Fine-tune the model (assuming you have a DataLoader for your dataset)\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Get embeddings and make recommendations\n",
    "with torch.no_grad():\n",
    "    user_input = inputs.to(device)\n",
    "    user_embedding = model.bert(user_input)['pooler_output']\n",
    "    \n",
    "    # Calculate car embeddings for all cars in the dataset\n",
    "    car_embeddings = []\n",
    "    for car_text in car_texts:\n",
    "        car_input = tokenizer(car_text, return_tensors=\"pt\").to(device)\n",
    "        car_embedding = model.bert(car_input)['pooler_output']\n",
    "        car_embeddings.append(car_embedding.squeeze().detach().cpu().numpy())\n",
    "    car_embeddings = np.stack(car_embeddings)\n",
    "    \n",
    "    # Calculate similarity scores between user preferences and car embeddings\n",
    "    similarity_scores = torch.matmul(user_embedding, torch.tensor(car_embeddings).T)\n",
    "    \n",
    "    # Sort cars by similarity scores\n",
    "    recommended_cars = torch.argsort(similarity_scores, descending=True).squeeze().tolist()\n",
    "    \n",
    "# Print top-k recommended cars\n",
    "top_k = 5\n",
    "for i in range(top_k):\n",
    "    print(f\"Rank {i+1}: Car {recommended_cars[i]}, Similarity Score: {similarity_scores.squeeze()[recommended_cars[i]]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User dataset\n",
    "\n",
    "http://users.cecs.anu.edu.au/~u4940058/CarPreferences.html\n",
    "\n",
    "User preference\n",
    "https://www.kaggle.com/datasets/steventaylor11/stated-preferences-for-car-choice?resource=download\n",
    "\n",
    "Methodology\n",
    "https://nycdatascience.com/blog/student-works/data-study-on-car-brand-preferences/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
