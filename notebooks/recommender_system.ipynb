{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from .autonotebook import tqdm as notebook_tqdm \n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# Encode input text\n",
    "input_text = \"The user prefers a red, 2023, automatic transmission, electric car with a budget of $50,000.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Fine-tune the model (assuming you have a DataLoader for your dataset)\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Get embeddings and make recommendations\n",
    "with torch.no_grad():\n",
    "    user_input = inputs.to(device)\n",
    "    user_embedding = model.bert(user_input)['pooler_output']\n",
    "    \n",
    "    # Calculate car embeddings for all cars in the dataset\n",
    "    car_embeddings = []\n",
    "    for car_text in car_texts:\n",
    "        car_input = tokenizer(car_text, return_tensors=\"pt\").to(device)\n",
    "        car_embedding = model.bert(car_input)['pooler_output']\n",
    "        car_embeddings.append(car_embedding.squeeze().detach().cpu().numpy())\n",
    "    car_embeddings = np.stack(car_embeddings)\n",
    "    \n",
    "    # Calculate similarity scores between user preferences and car embeddings\n",
    "    similarity_scores = torch.matmul(user_embedding, torch.tensor(car_embeddings).T)\n",
    "    \n",
    "    # Sort cars by similarity scores\n",
    "    recommended_cars = torch.argsort(similarity_scores, descending=True).squeeze().tolist()\n",
    "    \n",
    "# Print top-k recommended cars\n",
    "top_k = 5\n",
    "for i in range(top_k):\n",
    "    print(f\"Rank {i+1}: Car {recommended_cars[i]}, Similarity Score: {similarity_scores.squeeze()[recommended_cars[i]]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User dataset\n",
    "\n",
    "http://users.cecs.anu.edu.au/~u4940058/CarPreferences.html\n",
    "\n",
    "User preference\n",
    "https://www.kaggle.com/datasets/steventaylor11/stated-preferences-for-car-choice?resource=download\n",
    "\n",
    "Methodology\n",
    "https://nycdatascience.com/blog/student-works/data-study-on-car-brand-preferences/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
