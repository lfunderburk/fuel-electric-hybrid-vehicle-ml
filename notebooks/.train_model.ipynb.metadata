{"timestamp": 1681862446.674317, "stored_source_code": "# declare a list tasks whose products you want to use as inputs\nupstream = None\nimport pandas as pd\nimport sys, os\nfrom pathlib import Path\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import confusion_matrix, \\\n                            classification_report, \\\n                            accuracy_score,\\\n                            balanced_accuracy_score,\\\n                            ConfusionMatrixDisplay\nfrom sklearn.metrics import DetCurveDisplay, RocCurveDisplay\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nimport joblib\nimport utils\nTO DO: follow this tutorial to complete both smog and co2 ratings https://machinelearningmastery.com/knn-imputation-for-missing-values-in-machine-learning/\ndef train_and_evaluate_model(X_train, y_train, X_test, y_test, model_pipeline, model_name):\n    \"\"\"\n    This function trains and evaluates model, and generates confusion matrix, classification report, and accuracy score\n    Parameters:\n    ----------\n        X_train\n        y_train\n        X_test\n        y_test\n        model_pipeline\n        model_name\n    Returns:\n    -------\n        None\n    \"\"\"\n    \n    model_pipeline.fit(X_train, y_train.values.ravel())\n\n\n    # Predict\n    y_pred = model_pipeline.predict(X_test)\n    \n    # Obtain accuracy score\n    acc = accuracy_score(y_test, y_pred)\n    print('accuracy is',accuracy_score(y_pred,y_test))\n    \n    score_train = model_pipeline.score(X_train, y_train)\n    score_test = model_pipeline.score(X_test, y_test)\n    print('score for training set', score_train, 'score for testing set', score_test)\n    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n    print(\"Balanced accuracy score\", balanced_accuracy)\n    \n    report = classification_report(y_test, y_pred)\n    \n    fig, ax = plt.subplots(figsize=(10, 5))\n    ConfusionMatrixDisplay.from_predictions(y_test, y_pred, ax=ax)\n    _ = ax.set_title(\n        f\"Confusion Matrix for {model_name}\"\n    )\n    \n    # save image to file\n    fig.savefig(f\"./reports/{model_name}_confusion_matrix.png\")\n    \n    print(report, sep=',')\ndef classify_grid_search_cv_tuning(model, parameters, X_train, X_test, y_train, y_test, n_folds = 5, scoring='accuracy'):\n    \"\"\"\n    This function tunes GridSearchCV model\n    \n    Parameters:\n    ----------\n        model\n        parameters\n        X_train\n        X_test\n        y_train\n        y_test\n        n_folds\n        scoring\n        \n    Returns:\n    --------\n        best_model\n        best_score\n    \"\"\"\n    # Set up and fit model\n    tune_model = GridSearchCV(model, param_grid=parameters, cv=n_folds, scoring=scoring)\n    tune_model.fit(X_train, y_train.values.ravel())\n    \n    best_model = tune_model.best_estimator_\n    best_score = tune_model.best_score_\n    y_pred = best_model.predict(X_test)\n    \n    # Printing results\n    print(\"Best parameters:\", tune_model.best_params_)\n    print(\"Cross-validated accuracy score on training data: {:0.4f}\".format(tune_model.best_score_))\n    print()\n\n    print(classification_report(y_test, y_pred))\n    \n    return best_model, best_score\nif __name__==\"__main__\":\n\n    # Set up paths\n    sys.path.append(os.path.abspath(os.path.join('./data/', './processed/')))\n    sys.path.append(os.path.abspath(os.path.join('./models/')))\n\n\n    # Read data\n    fuel_df, electric_df, hybrid_df = utils.read_data(\"./data/processed/\")\n    non_na_rating_class, na_rating_class = utils.remove_missing_values(fuel_df, drop_smog=False)\n    \n    # Set X and Y variables \n    # Response variable\n    Y = non_na_rating_class[['co2_rating']]\n\n    # Dependent variables\n    X = non_na_rating_class[utils.var_list]\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n    \n    \n    # Set up pipeline\n    # Set up parameters for the model - numerical and categorical\n    numeric_features =  utils.numeric_features\n    categorical_features = utils.categorical_features\n\n    # Use smote to balance the data\n    smote = SMOTE(random_state=42)\n    X_train, y_train = smote.fit_resample(X_train[numeric_features], y_train)\n\n    # Set up preprocessor\n    preprocessor = utils.preprocessor\n\n    # Set up model pipeline\n    clf1 = KNeighborsClassifier(3,)\n    clf2 = SVC(gamma=2, C=1, random_state=42)\n    clf3 = RandomForestClassifier(max_depth=100, n_estimators=10, max_features=1, random_state=42)\n\n    classifiers = {\"KNN\": clf1, \n                   \"SVM\": clf2,\n                   \"RFC\": clf3\n                }\n\n    eclf1 = VotingClassifier(estimators=[('knn', clf1), ('svm', clf2), ('dt', clf3)], voting='hard')\n    model = Pipeline(\n            steps=[(\"preprocessor\", preprocessor), \n                    (\"hard Voting\", eclf1 )] #colsample  by tree, n estimators, max depth\n                                                                        )\n    train_and_evaluate_model(X_train, y_train, X_test, y_test, model,\"Voting\")\n\n    params = {}\n    best_dtc, dtc_score = classify_grid_search_cv_tuning(model, params, X_train, X_test, y_train, y_test, n_folds=10, scoring='balanced_accuracy')\n\n    # Save model\n    joblib.dump(best_dtc, './models/hard_voting_classifier_co2_fuel.pkl')", "params": {}}